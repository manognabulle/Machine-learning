{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mY0bEC38SzT1",
        "outputId": "c8574fdd-319c-4d6c-bdac-3cc4bc044a88"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
            "Best Parameters: {'n_estimators': 300, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': None}\n",
            "Accuracy: 0.8883928571428571\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.98      0.94       381\n",
            "           1       0.79      0.34      0.48        67\n",
            "\n",
            "    accuracy                           0.89       448\n",
            "   macro avg       0.84      0.66      0.71       448\n",
            "weighted avg       0.88      0.89      0.87       448\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Load dataset\n",
        "file_path = 'MC.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Handle missing values (fill income with median)\n",
        "df['Income'] = df['Income'].fillna(df['Income'].median())\n",
        "\n",
        "# Drop ID and Dt_Customer (not useful for ML)\n",
        "df = df.drop(['ID', 'Dt_Customer'], axis=1)\n",
        "\n",
        "# Encode categorical variables\n",
        "le = LabelEncoder()\n",
        "for col in ['Education', 'Marital_Status']:\n",
        "    df[col] = le.fit_transform(df[col])\n",
        "\n",
        "# Features and target\n",
        "X = df.drop('Response', axis=1)\n",
        "y = df['Response']\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Random Forest Model\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Hyperparameter grid\n",
        "param_dist = {\n",
        "    'n_estimators': [100, 200, 300, 500],\n",
        "    'max_depth': [None, 10, 20, 30, 50],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4],\n",
        "    'max_features': ['sqrt', 'log2', None]\n",
        "}\n",
        "\n",
        "# RandomizedSearchCV\n",
        "random_search = RandomizedSearchCV(\n",
        "    estimator=rf,\n",
        "    param_distributions=param_dist,\n",
        "    n_iter=20,  # number of parameter settings sampled\n",
        "    cv=5,\n",
        "    verbose=2,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# Fit model\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# Best parameters\n",
        "print(\"Best Parameters:\", random_search.best_params_)\n",
        "\n",
        "# Evaluate on test set\n",
        "y_pred = random_search.predict(X_test)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Classifiers\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from catboost import CatBoostClassifier\n",
        "\n",
        "# Load dataset\n",
        "file_path = 'MC.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Handle missing values\n",
        "df['Income'] = df['Income'].fillna(df['Income'].median())\n",
        "\n",
        "# Drop ID and Dt_Customer\n",
        "df = df.drop(['ID', 'Dt_Customer'], axis=1)\n",
        "\n",
        "# Encode categorical variables\n",
        "le = LabelEncoder()\n",
        "for col in ['Education', 'Marital_Status']:\n",
        "    df[col] = le.fit_transform(df[col])\n",
        "\n",
        "# Features and target\n",
        "X = df.drop('Response', axis=1)\n",
        "y = df['Response']\n",
        "\n",
        "# Scale features (important for SVM & MLP)\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Models to evaluate\n",
        "models = {\n",
        "    'SVM': SVC(kernel='rbf', probability=True, random_state=42),\n",
        "    'DecisionTree': DecisionTreeClassifier(random_state=42),\n",
        "    'RandomForest': RandomForestClassifier(random_state=42),\n",
        "    'AdaBoost': AdaBoostClassifier(random_state=42),\n",
        "    'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42),\n",
        "    'CatBoost': CatBoostClassifier(verbose=0, random_state=42),\n",
        "    'NaiveBayes': GaussianNB(),\n",
        "    'MLP': MLPClassifier(max_iter=500, random_state=42)\n",
        "}\n",
        "\n",
        "# Function to evaluate models\n",
        "def evaluate_model(model, X_train, y_train, X_test, y_test):\n",
        "    model.fit(X_train, y_train)\n",
        "    y_train_pred = model.predict(X_train)\n",
        "    y_test_pred = model.predict(X_test)\n",
        "\n",
        "    metrics = {\n",
        "        'Train_Accuracy': accuracy_score(y_train, y_train_pred),\n",
        "        'Test_Accuracy': accuracy_score(y_test, y_test_pred),\n",
        "        'Precision': precision_score(y_test, y_test_pred, zero_division=0),\n",
        "        'Recall': recall_score(y_test, y_test_pred, zero_division=0),\n",
        "        'F1_Score': f1_score(y_test, y_test_pred, zero_division=0)\n",
        "    }\n",
        "    return metrics\n",
        "\n",
        "# Evaluate all models\n",
        "results = {}\n",
        "for name, model in models.items():\n",
        "    results[name] = evaluate_model(model, X_train, y_train, X_test, y_test)\n",
        "\n",
        "# Convert results to DataFrame\n",
        "results_df = pd.DataFrame(results).T\n",
        "print(results_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CKGz3cS9TcY0",
        "outputId": "660fe127-7717-4288-b1c3-810e18352c9a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:183: UserWarning: [18:10:25] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              Train_Accuracy  Test_Accuracy  Precision    Recall  F1_Score\n",
            "SVM                 0.911272       0.881696   0.791667  0.283582  0.417582\n",
            "DecisionTree        0.994978       0.830357   0.432836  0.432836  0.432836\n",
            "RandomForest        0.994978       0.883929   0.800000  0.298507  0.434783\n",
            "AdaBoost            0.889509       0.875000   0.703704  0.283582  0.404255\n",
            "XGBoost             0.994420       0.881696   0.705882  0.358209  0.475248\n",
            "CatBoost            0.974888       0.886161   0.785714  0.328358  0.463158\n",
            "NaiveBayes          0.832589       0.808036   0.388235  0.492537  0.434211\n",
            "MLP                 0.986049       0.859375   0.538462  0.417910  0.470588\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "# Regressors\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from xgboost import XGBRegressor\n",
        "\n",
        "# Load dataset\n",
        "file_path = 'MC.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Handle missing values\n",
        "df['Income'] = df['Income'].fillna(df['Income'].median())\n",
        "\n",
        "# Drop ID and Dt_Customer\n",
        "df = df.drop(['ID', 'Dt_Customer'], axis=1)\n",
        "\n",
        "# Encode categorical variables\n",
        "le = LabelEncoder()\n",
        "for col in ['Education', 'Marital_Status']:\n",
        "    df[col] = le.fit_transform(df[col])\n",
        "\n",
        "# Features and target (regression: predict Income)\n",
        "X = df.drop('Income', axis=1)\n",
        "y = df['Income']\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Models\n",
        "models = {\n",
        "    'SVR': SVR(),\n",
        "    'DecisionTreeRegressor': DecisionTreeRegressor(random_state=42),\n",
        "    'RandomForestRegressor': RandomForestRegressor(random_state=42),\n",
        "    'AdaBoostRegressor': AdaBoostRegressor(random_state=42),\n",
        "    'XGBRegressor': XGBRegressor(random_state=42, objective='reg:squarederror'),\n",
        "    'MLPRegressor': MLPRegressor(max_iter=500, random_state=42)\n",
        "}\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate_regressor(model, X_train, y_train, X_test, y_test):\n",
        "    model.fit(X_train, y_train)\n",
        "    y_train_pred = model.predict(X_train)\n",
        "    y_test_pred = model.predict(X_test)\n",
        "    metrics = {\n",
        "        'Train_R2': r2_score(y_train, y_train_pred),\n",
        "        'Test_R2': r2_score(y_test, y_test_pred),\n",
        "        'MAE': mean_absolute_error(y_test, y_test_pred),\n",
        "        'MSE': mean_squared_error(y_test, y_test_pred),\n",
        "        'RMSE': np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
        "    }\n",
        "    return metrics\n",
        "\n",
        "# Evaluate\n",
        "results = {}\n",
        "for name, model in models.items():\n",
        "    results[name] = evaluate_regressor(model, X_train, y_train, X_test, y_test)\n",
        "\n",
        "# Results DataFrame\n",
        "results_df = pd.DataFrame(results).T\n",
        "print(results_df)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J-Qk-AxUUiQQ",
        "outputId": "e94e3b14-3839-4910-c9a3-7c21228c5faa"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                       Train_R2   Test_R2           MAE           MSE  \\\n",
            "SVR                    0.006159  0.000060  17076.205580  4.261488e+08   \n",
            "DecisionTreeRegressor  0.999624 -3.413130   9264.301339  1.880764e+09   \n",
            "RandomForestRegressor  0.935966  0.851522   5461.710110  6.327757e+07   \n",
            "AdaBoostRegressor      0.835005  0.701323   8997.720944  1.272885e+08   \n",
            "XGBRegressor           0.998378  0.808494   5443.896064  8.161515e+07   \n",
            "MLPRegressor          -2.033291 -3.549796  40924.296698  1.939007e+09   \n",
            "\n",
            "                               RMSE  \n",
            "SVR                    20643.371901  \n",
            "DecisionTreeRegressor  43367.773683  \n",
            "RandomForestRegressor   7954.719880  \n",
            "AdaBoostRegressor      11282.219084  \n",
            "XGBRegressor            9034.110147  \n",
            "MLPRegressor           44034.157679  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.cluster import AgglomerativeClustering, DBSCAN, OPTICS\n",
        "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv(\"MC.csv\")\n",
        "\n",
        "# Drop target column if exists (last column)\n",
        "X = df.drop(columns=[df.columns[-1]])\n",
        "\n",
        "# Convert categorical columns to numeric using One-Hot Encoding\n",
        "X = pd.get_dummies(X, drop_first=True)\n",
        "\n",
        "# Handle missing values (impute with mean for numeric features)\n",
        "imputer = SimpleImputer(strategy=\"mean\")\n",
        "X_imputed = imputer.fit_transform(X)\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X_imputed)\n",
        "\n",
        "# Dictionary of clustering algorithms\n",
        "clustering_algorithms = {\n",
        "    \"Agglomerative (Ward)\": AgglomerativeClustering(n_clusters=3, linkage='ward'),\n",
        "    \"Agglomerative (Average)\": AgglomerativeClustering(n_clusters=3, linkage='average'),\n",
        "    \"DBSCAN\": DBSCAN(eps=0.5, min_samples=5),\n",
        "    \"OPTICS\": OPTICS(min_samples=5)\n",
        "}\n",
        "\n",
        "# Results storage\n",
        "results = []\n",
        "\n",
        "for name, model in clustering_algorithms.items():\n",
        "    labels = model.fit_predict(X_scaled)\n",
        "\n",
        "    # Handle cases where clustering fails (all points in one cluster or all noise)\n",
        "    if len(set(labels)) > 1 and len(set(labels)) < len(X_scaled):\n",
        "        silhouette = silhouette_score(X_scaled, labels)\n",
        "        db_index = davies_bouldin_score(X_scaled, labels)\n",
        "        ch_index = calinski_harabasz_score(X_scaled, labels)\n",
        "    else:\n",
        "        silhouette, db_index, ch_index = None, None, None\n",
        "\n",
        "    results.append({\n",
        "        \"Model\": name,\n",
        "        \"Silhouette Score\": silhouette,\n",
        "        \"Davies-Bouldin\": db_index,\n",
        "        \"Calinski-Harabasz\": ch_index\n",
        "    })\n",
        "\n",
        "# Convert to DataFrame\n",
        "results_df = pd.DataFrame(results)\n",
        "print(results_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZU03SkZTU5OH",
        "outputId": "445daf53-86cb-47a5-d891-49066a0fb818"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                     Model  Silhouette Score  Davies-Bouldin  \\\n",
            "0     Agglomerative (Ward)          0.003862       16.064879   \n",
            "1  Agglomerative (Average)          0.342164        0.520901   \n",
            "2                   DBSCAN               NaN             NaN   \n",
            "3                   OPTICS          0.176422        1.756949   \n",
            "\n",
            "   Calinski-Harabasz  \n",
            "0           5.667667  \n",
            "1           3.418561  \n",
            "2                NaN  \n",
            "3           3.961352  \n"
          ]
        }
      ]
    }
  ]
}